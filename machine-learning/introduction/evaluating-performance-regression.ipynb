{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluating Performance for Regression Tests\n",
    "-Regression is a tank when a model attempts to predict continuous values (unlike categorical values, which is classification)\n",
    "-Can't use accuracy of recall for regression because we need metrics designed for continuous values\n",
    "Some examples: predicting the price of a house given its features\n",
    "Most common evaluatiopn metrics for regression:\n",
    "-Mean absolute error (MAE) - mean of the absolute value of errors. Take the absolute value of (True price (y) - predicted price(y-hat))\n",
    "    This does not punish large errors.\n",
    "-Mean squared error (MSE) - Larger errors are noted more than with MAE, making MSE more popular\n",
    "    (y - yhat)^2 \n",
    "    The problem that can happen is that squaring the label also squares the units as well. To fix this, you use Root Mean Error\n",
    "-Root Mean Error(RMSE)\n",
    "    - This is the root of the mean of the squared errors\n",
    "    -Most popular (has same units as y)\n",
    "    sqrt((y - yhat)^2 )\n",
    "\n",
    "**To know if your performance is great, compare your error metric to the average value of the label in your data set**\n",
    "'''"
   ]
  }
 ]
}