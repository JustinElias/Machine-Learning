{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Key classification metrics:\n",
    "-accuracy - One of the most common classification metrics. The number of correct predictions made by the model/ total number of predictions. Good to use when the target classes are well balanced. If you are predicting cats or dogs -- we would have roughly the same amount of cat images as we have dog images. Do not use if there are unbalanced classes. If we had 99 images of dogs and 1 image of a cat. \n",
    "-recall - ability of a model to find all the relevant cases within a dataset. (The number of true positives)/(the number of true positives + number of false negatives)\n",
    "-precision - ability of a classification model to identify only the relevant data points. (Number of true positives)/(number of true positives + number of false positives)\n",
    "**There is normally a trade off between Recall and Precision. Recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant**\n",
    "-F1-score - used in cases where we want to find an optimal blend of precision and recall we can combine the two metrics using F1 score. F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation: f1 = 2* ((precision*recall)/(precision + recall))\n",
    "Using the harmonic mean helps deal with extreme values. For example, if you had a classifier with precision 1.0 and a recall of 0.0, the simple average is 0.5, but the F1 score is 0.\n",
    "\n",
    "Your model can either be correct in its prediction or incorrect in its prediction. However, the incorrect vs correct expand to have multiple different classes\n",
    "\n",
    "Binary Classification example (2 options)\n",
    "Is an image a dog or a cat?\n",
    "\n",
    "First step: fit/train a model on training data, then test the model with training data\n",
    "Because this is a supervised learning example, someone will look at pictures and label it that it's a cat or a dog\n",
    "Once we have the model's predictions from the X_test data, we compare it to the true y values (the correct labels)\n",
    "\n",
    "Picture of a dog with the label dog -> Trained Model -> predicts it is a dog\n",
    "Compare the prediction to the correct label. If it predicted it was a cat, it was wrong\n",
    "Repeat this project for all images in the X test data. At the end, we will have a count of correct matches and a count of incorrect matches\n",
    "The key realization we need to make, is that in the real world, not all incorrect or correct matches hold equal value\n",
    "\n",
    "Confusion Matrix:\n",
    "see root of folder for a picture\n",
    "\n",
    "Model Evaluation\n",
    "When we have the precision/recall trade off, it needs to be decided on if the model will focus on fixing false positives vs false negatives\n",
    "In disease diagnosis, it is probably better to go in the direction of false positives, so we can make sure we correctly classify as many cases of disease as possible\n",
    "False Negative - turning someone away who actually has a disease and we tell them the prediction says they don't\n",
    "False Positive - telling someone they have a disease, but finding out they don't when they do a biopse\n",
    "'''"
   ]
  }
 ]
}